static void mm_alloc_slabs(struct mm_slab_cache* cache) {
    serial_puts("\n[SLAB] mm_alloc_slabs: Finding last slab...");
    struct mm_slab* start = cache->slabs_free;
    if (start != NULL)  {
        while (start->next != NULL) {
            start = start->next;
        }
    }

    serial_puts("\n[SLAB] mm_alloc_slabs: Found last slab.");

    if (start != NULL) {
        //mm_slab_debug_slabs_no_next(start);
        //serial_puts("\n[SLAB] mm_alloc_slabs: First slab is not NULL.");
        start->next = (struct mm_slab*)(pmm_alloc(DIV_ROUND_UP(cache->batchcount * sizeof(struct mm_slab), PMM_PAGE_SIZE)) + __NONE);
        memsetb((uint8_t*)start->next, 0, PMM_PAGE_SIZE * DIV_ROUND_UP(cache->batchcount * sizeof(struct mm_slab), PMM_PAGE_SIZE));
        //serial_puts("\n\n\n ADDR OF START->NEXT: ");
        //serial_puts(unsigned_long_to_str((uint64_t)start->next));

        if (!vmm_page_is_mapped((uint64_t)start->next)) {
            vmm_map_page((uint64_t)start->next, (uint64_t)start->next - __NONE, 0x3);
        }

        for (size_t i = 0; i < cache->batchcount; i++) {
            start->next->parent_cache = cache;
            start->next->free = cache->batchcount;
            start->next->inuse = 0;
            start->next->next = NULL;
            start->next->last = start;

            start->next->buff = (void*)(pmm_alloc(DIV_ROUND_UP(cache->batchcount * sizeof(struct mm_slab), PMM_PAGE_SIZE)) + __NONE);
            memsetb(start->next->buff, 0, PMM_PAGE_SIZE * DIV_ROUND_UP(cache->batchcount * sizeof(struct mm_slab), PMM_PAGE_SIZE));
            start->next->bitmap = (uint8_t*)(pmm_alloc(DIV_ROUND_UP(DIV_ROUND_UP(cache->object_size * cache->batchcount, 8), PMM_PAGE_SIZE)) + __NONE);
            memsetb(start->next->bitmap, 0, PMM_PAGE_SIZE * DIV_ROUND_UP(DIV_ROUND_UP(cache->object_size * cache->batchcount, 8), PMM_PAGE_SIZE));

            if (!vmm_page_is_mapped((uint64_t)start->next->buff)) {
                vmm_map_page((uint64_t)start->next->buff, (uint64_t)start->next->buff - __NONE, 0x3);
            }

            if (!vmm_page_is_mapped((uint64_t)start->next->bitmap)) {
                vmm_map_page((uint64_t)start->next->bitmap, (uint64_t)start->next->bitmap - __NONE, 0x3);
            }

            if (i+1 >= cache->batchcount) {
                //serial_puts("\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA END.....");
                start->next->next = NULL;
                mm_slab_debug_slabs_no_next(start->next);
                break;
            }
            start->next->next = (struct mm_slab*)((uint64_t)start->next + sizeof(struct mm_slab));
            mm_slab_debug_slabs_no_next(start->next);
            start = start->next;
        }
    } else {
        //serial_puts("\n[SLAB] mm_alloc_slabs: First slab is NULL.");
        start = (struct mm_slab*)(pmm_alloc(DIV_ROUND_UP(cache->batchcount * sizeof(struct mm_slab), PMM_PAGE_SIZE)) + __NONE);
        memsetb((uint8_t*)start, 0, PMM_PAGE_SIZE * DIV_ROUND_UP(cache->batchcount * sizeof(struct mm_slab), PMM_PAGE_SIZE));

        //serial_puts("\n[SLAB] mm_alloc_slabs: Memset bytes - ");
        //serial_puts(unsigned_long_to_str(PMM_PAGE_SIZE * DIV_ROUND_UP(cache->batchcount * sizeof(struct mm_slab), PMM_PAGE_SIZE)));
        //serial_puts("\n[SLAB] mm_alloc_slabs: Slab start alloc buffer size: ");
        //serial_puts(unsigned_long_to_str(DIV_ROUND_UP(cache->batchcount * sizeof(struct mm_slab), PMM_PAGE_SIZE)));
        //serial_puts("\n[SLAB] mm_alloc_slabs: Slab address: ");
        //serial_puts(unsigned_long_to_str((uint64_t)start));

        if (!vmm_page_is_mapped((uint64_t)start)) {
            //serial_puts("\n[SLAB] ERROR: Page is not mapped. Mapping now...");
            vmm_map_page((uint64_t)start, (uint64_t)start - __NONE, 0x3);
        }

        cache->slabs_free = start;

        for (size_t i = 0; i < cache->batchcount; i++) {
            //serial_puts("\n - [+] Allocating slab ");
            //serial_puts(unsigned_long_to_str(i+1));
            //serial_puts("/");
            //serial_puts(unsigned_long_to_str(cache->batchcount));
            start->parent_cache = cache;
            start->free = cache->batchcount;
            start->inuse = 0;
            start->next = NULL;
            start->last = (struct mm_slab*)((uint64_t)start - sizeof(struct mm_slab));
            if (i == 0) {
                start->last = NULL;
            }

            start->buff = (void*)(pmm_alloc(DIV_ROUND_UP(cache->batchcount * sizeof(struct mm_slab), PMM_PAGE_SIZE)) + __NONE);
            memsetb(start->buff, 0, PMM_PAGE_SIZE * DIV_ROUND_UP(cache->batchcount * sizeof(struct mm_slab), PMM_PAGE_SIZE));
            start->bitmap = (uint8_t*)(pmm_alloc(DIV_ROUND_UP(DIV_ROUND_UP(cache->object_size * cache->batchcount, 8), PMM_PAGE_SIZE)) + __NONE);
            memsetb(start->bitmap, 0, PMM_PAGE_SIZE * DIV_ROUND_UP(DIV_ROUND_UP(cache->object_size * cache->batchcount, 8), PMM_PAGE_SIZE));

            if (!vmm_page_is_mapped((uint64_t)start->buff)) {
                vmm_map_page((uint64_t)start->buff, (uint64_t)start->buff - __NONE, 0x3);
            }

            if (!vmm_page_is_mapped((uint64_t)start->bitmap)) {
                vmm_map_page((uint64_t)start->bitmap, (uint64_t)start->bitmap - __NONE, 0x3);
            }

            if (i+1 >= cache->batchcount) {
                start->next->next = NULL;
                break;
            }
            start->next = (struct mm_slab*)((uint64_t)start + sizeof(struct mm_slab));
            start = start->next;
        }
    }

    //serial_puts("\n[SLAB] mm_alloc_slabs: FINAL DEBUG!");
    mm_slab_debug_slabs(cache->slabs_free);
}